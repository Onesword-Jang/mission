{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c55bdb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key 입력: ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# OpenAI API Key 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key 입력: \")\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d005c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 파일 경로 설정\n",
    "result_folder = \"./\"  # 동일 폴더에 있는 경우\n",
    "result_files = [\"result1.txt\", \"result2.txt\", \"result3.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21e2efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 파일 내용 읽기\n",
    "results = {}\n",
    "for file_name in result_files:\n",
    "    file_path = os.path.join(result_folder, file_name)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        results[file_name] = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04458478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 및 결과 매핑\n",
    "Question = \"Keep me up to date on the latest research on supersized language models\"  # 사용자 질문\n",
    "answer_a = results.get(\"result1.txt\", \"No answer found.\")\n",
    "answer_b = results.get(\"result2.txt\", \"No answer found.\")\n",
    "answer_c = results.get(\"result3.txt\", \"No answer found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3b4848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation prompt\n",
    "Prompt = f\"\"\"[System]\n",
    "Be an impartial judge and rate the quality of the answers provided by the two people.\n",
    "Rate the quality of the answers provided by the AI assistants. You should choose an assistant that follows your instructions better and answers your questions better. \n",
    "Rating criteria include usefulness, relevance, accuracy, depth, and creativity of the answer, as well as the level of detail in the answer.\n",
    "\n",
    "Start your evaluation by comparing three answers. Compare the responses and provide a brief explanation. Avoid any position bias and ensure the following:\n",
    "- Don't let the order of the answers influence your decision.\n",
    "- Don't let the length of the responses influence your evaluation.\n",
    "- Don't favor any specific name or assistant. Be as objective as possible.\n",
    "\n",
    "After providing your comments, strictly adhere to the following format:\n",
    "Output your final rating: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, \"[[C]]\" if assistant C is better, or \"[[D]]\" if there is a tie.\n",
    "\n",
    "[User Question]\n",
    "{Question}\n",
    "\n",
    "[start of Assistant A's answer]\n",
    "{answer_a}\n",
    "[end of Assistant A's answer]\n",
    "\n",
    "[start of Assistant B's answer]\n",
    "{answer_b}\n",
    "[end of Assistant B's answer]\n",
    "\n",
    "[start of Assistant C's answer]\n",
    "{answer_c}\n",
    "[end of Assistant C's answer]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "281a1786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Result:\n",
      "All three assistants provided comprehensive overviews of the latest research trends in supersized language models, but there are some differences in their responses:\n",
      "\n",
      "1. **Assistant A**: This response is detailed and covers a wide range of topics, including model size and performance, efficiency and sustainability, multimodal learning, ethical considerations, transfer learning and fine-tuning, user customization, and model interpretability. The answer is well-structured and provides a thorough overview of the current research landscape.\n",
      "\n",
      "2. **Assistant B**: This response also covers several key areas such as model size and performance, efficient learning methods, safety and ethics, various applications, and model interpretability. However, it is slightly less detailed than Assistant A's response and does not mention some specific techniques or trends like multimodal learning or user customization.\n",
      "\n",
      "3. **Assistant C**: This response focuses on efficiency improvements, continual learning and adaptability, ethical considerations, multimodal learning, and resource consumption. It introduces specific techniques like Sparse Attention and Mixture of Experts, which adds depth to the discussion on efficiency. However, it does not cover as many topics as Assistant A, such as user customization or transfer learning.\n",
      "\n",
      "In terms of usefulness, relevance, accuracy, depth, and creativity, Assistant A provides the most comprehensive and detailed response, covering a broader range of topics and offering a more complete picture of the current research trends in supersized language models.\n",
      "\n",
      "Output your final rating: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, \"[[C]]\" if assistant C is better, or \"[[D]]\" if there is a tie.\n"
     ]
    }
   ],
   "source": [
    "# openapi 호출\n",
    "completion= client.chat.completions.create(\n",
    "    model = 'gpt-4o',\n",
    "    messages=[{'role':'user','content':Prompt}],\n",
    "    temperature = 0.0\n",
    ")\n",
    "\n",
    "# 응답 출력\n",
    "print(\"Evaluation Result:\")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e971248",
   "metadata": {},
   "source": [
    "# 결과 설명\n",
    "## 평가 결과:\n",
    "- 세 조교 모두 초대형 언어 모델의 최신 연구 동향에 대해 포괄적인 개요를 제공했지만 응답에는 약간의 차이가 있었습니다:\n",
    "\n",
    " 1. **A** 어시스턴트: 모델 크기와 성능, 효율성 및 지속 가능성, 다중 모드 학습, 윤리적 고려 사항, 전이 학습 및 미세 조정, 사용자 맞춤화, 모델 해석 가능성 등 다양한 주제를 상세하게 다루고 있습니다. 답변은 체계적으로 구성되어 있으며 현재 연구 환경에 대한 철저한 개요를 제공합니다.\n",
    "\n",
    " 2. **보조 B**: 이 답변도 모델 크기 및 성능, 효율적인 학습 방법, 안전 및 윤리, 다양한 응용 분야, 모델 해석 가능성 등 몇 가지 핵심 영역을 다루고 있습니다. 하지만 A 어시스턴트의 답변보다 약간 덜 상세하며, 멀티모달 학습이나 사용자 맞춤화와 같은 특정 기술이나 트렌드에 대해서는 언급하지 않았습니다.\n",
    "\n",
    " 3. **어시스턴트 C**: 이 답변은 효율성 향상, 지속적인 학습 및 적응성, 윤리적 고려 사항, 멀티모달 학습 및 리소스 소비에 중점을 두고 있습니다. 드문 주의와 전문가 혼합과 같은 구체적인 기법을 소개하여 효율성에 대한 논의에 깊이를 더합니다. 하지만 사용자 맞춤 설정이나 전이 학습과 같이 어시스턴트 A만큼 많은 주제를 다루지는 않습니다.\n",
    "\n",
    "- 유용성, 관련성, 정확성, 깊이, 창의성 측면에서 어시스턴트 A가 가장 포괄적이고 상세한 답변을 제공하며 더 광범위한 주제를 다루고 초대형 언어 모델의 현재 연구 동향에 대한 보다 완전한 그림을 제공합니다.\n",
    "\n",
    "- 최종 평점을 출력합니다: \"어시스턴트 A가 더 우수하면 '[[A]]', 어시스턴트 B가 더 우수하면 '[[B]]', 어시스턴트 C가 더 우수하면 '[[C]]', 동점인 경우 '[[D]]'로 표시합니다.\n",
    "\n",
    "## 한줄로 정의한 프롬프트의 평가가 가장 좋다고 나옴\n",
    "\n",
    "## 마지막 줄에서 이상한 말을 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c23365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
