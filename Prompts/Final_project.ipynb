{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f078446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key 입력: ········\n",
      "사용할 프롬프트를 선택하세요:\n",
      "1: prompt1.txt\n",
      "2: prompt2.txt\n",
      "3: prompt3.txt\n",
      "선택: 3\n",
      "질문을 입력하세요: 초거대 언어모델 최근 주요 연구 동향을 알려줘\n",
      "Debug Output: content='질문의 의도는 최근 초거대 언어모델(Super Large Language Models) 분야에서의 주요 연구 동향을 파악하고자 하는 것으로 보입니다. 이는 연구자들이 현재의 기술 발전을 이해하고, 향후 연구 방향을 설정하는 데 도움이 될 수 있습니다. \\n\\n최근 초거대 언어모델에 대한 연구 동향은 다음과 같은 몇 가지 주요 주제로 요약될 수 있습니다:\\n\\n1. **모델의 효율성 향상**: 많은 연구자들이 모델의 크기와 성능 간의 균형을 맞추기 위해 효율적인 아키텍처를 개발하고 있습니다. 예를 들어, Sparse Attention, Mixture of Experts(MoE)와 같은 기법이 연구되고 있습니다. 이러한 기법들은 모델의 파라미터 수를 줄이면서도 성능을 유지하거나 향상시키는 데 기여하고 있습니다.\\n\\n2. **지속적인 학습과 적응성**: 초거대 언어모델이 새로운 데이터에 적응할 수 있도록 하는 연구가 활발히 진행되고 있습니다. 이는 모델이 훈련 후에도 지속적으로 학습할 수 있는 방법을 모색하는 것으로, 예를 들어, 메타 학습이나 온라인 학습 기법이 포함됩니다.\\n\\n3. **윤리적 고려사항 및 편향 문제**: 초거대 언어모델이 사회에 미치는 영향에 대한 연구가 증가하고 있습니다. 모델의 편향 문제를 해결하고, 윤리적인 AI 개발을 위한 가이드라인을 제시하는 연구가 활발히 이루어지고 있습니다.\\n\\n4. **멀티모달 학습**: 텍스트뿐만 아니라 이미지, 비디오 등 다양한 형태의 데이터를 처리할 수 있는 멀티모달 모델에 대한 연구가 증가하고 있습니다. 이러한 모델은 다양한 입력을 통합하여 더 풍부한 정보를 제공할 수 있습니다.\\n\\n5. **자원 소모와 환경적 영향**: 초거대 언어모델의 훈련 및 운영에 따른 에너지 소비와 환경적 영향을 줄이기 위한 연구도 활발히 진행되고 있습니다. 이는 지속 가능한 AI 개발을 위한 중요한 이슈로 대두되고 있습니다.\\n\\n이러한 동향들은 초거대 언어모델의 발전 방향을 제시하며, 연구자들이 향후 연구를 진행하는 데 있어 중요한 참고자료가 될 것입니다. 추가적인 정보나 구체적인 연구 사례가 필요하다면, 더 깊이 있는 자료를 찾아보겠습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 528, 'prompt_tokens': 127, 'total_tokens': 655, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_3de1288069', 'finish_reason': 'stop', 'logprobs': None} id='run-3e19d47a-6776-40b8-8879-cf9545ecc6f8-0' usage_metadata={'input_tokens': 127, 'output_tokens': 528, 'total_tokens': 655, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Final Response: \n",
      "질문의 의도는 최근 초거대 언어모델(Super Large Language Models) 분야에서의 주요 연구 동향을 파악하고자 하는 것으로 보입니다. 이는 연구자들이 현재의 기술 발전을 이해하고, 향후 연구 방향을 설정하는 데 도움이 될 수 있습니다. \n",
      "\n",
      "최근 초거대 언어모델에 대한 연구 동향은 다음과 같은 몇 가지 주요 주제로 요약될 수 있습니다:\n",
      "\n",
      "1. **모델의 효율성 향상**: 많은 연구자들이 모델의 크기와 성능 간의 균형을 맞추기 위해 효율적인 아키텍처를 개발하고 있습니다. 예를 들어, Sparse Attention, Mixture of Experts(MoE)와 같은 기법이 연구되고 있습니다. 이러한 기법들은 모델의 파라미터 수를 줄이면서도 성능을 유지하거나 향상시키는 데 기여하고 있습니다.\n",
      "\n",
      "2. **지속적인 학습과 적응성**: 초거대 언어모델이 새로운 데이터에 적응할 수 있도록 하는 연구가 활발히 진행되고 있습니다. 이는 모델이 훈련 후에도 지속적으로 학습할 수 있는 방법을 모색하는 것으로, 예를 들어, 메타 학습이나 온라인 학습 기법이 포함됩니다.\n",
      "\n",
      "3. **윤리적 고려사항 및 편향 문제**: 초거대 언어모델이 사회에 미치는 영향에 대한 연구가 증가하고 있습니다. 모델의 편향 문제를 해결하고, 윤리적인 AI 개발을 위한 가이드라인을 제시하는 연구가 활발히 이루어지고 있습니다.\n",
      "\n",
      "4. **멀티모달 학습**: 텍스트뿐만 아니라 이미지, 비디오 등 다양한 형태의 데이터를 처리할 수 있는 멀티모달 모델에 대한 연구가 증가하고 있습니다. 이러한 모델은 다양한 입력을 통합하여 더 풍부한 정보를 제공할 수 있습니다.\n",
      "\n",
      "5. **자원 소모와 환경적 영향**: 초거대 언어모델의 훈련 및 운영에 따른 에너지 소비와 환경적 영향을 줄이기 위한 연구도 활발히 진행되고 있습니다. 이는 지속 가능한 AI 개발을 위한 중요한 이슈로 대두되고 있습니다.\n",
      "\n",
      "이러한 동향들은 초거대 언어모델의 발전 방향을 제시하며, 연구자들이 향후 연구를 진행하는 데 있어 중요한 참고자료가 될 것입니다. 추가적인 정보나 구체적인 연구 사례가 필요하다면, 더 깊이 있는 자료를 찾아보겠습니다.\n",
      "답변이 다음 위치에 저장되었습니다: C:\\Users\\USER\\mission\\mission\\Results\\response_20241119_174131.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# OpenAI API Key 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key 입력: \")\n",
    "\n",
    "# 모델 로드\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3, max_tokens=1000)\n",
    "\n",
    "# PDF 파일 로드\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = \"초거대언어모델연구동향.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서 청크 나누기\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = recursive_text_splitter.split_documents(docs)\n",
    "\n",
    "# 벡터 임베딩 생성\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vector_dim = len(embeddings.embed_query(\"example text\"))\n",
    "index = faiss.IndexFlatL2(vector_dim)\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "# 문서 추가\n",
    "for split in splits:\n",
    "    vector_store.add_texts([split.page_content], metadatas=[split.metadata])\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "# 프롬프트 파일 선택\n",
    "def select_prompt():\n",
    "    prompts = {\n",
    "        \"1\": \"prompt1.txt\",\n",
    "        \"2\": \"prompt2.txt\",\n",
    "        \"3\": \"prompt3.txt\",\n",
    "    }\n",
    "\n",
    "    print(\"사용할 프롬프트를 선택하세요:\")\n",
    "    for key, value in prompts.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    choice = input(\"선택: \").strip()\n",
    "    return prompts.get(choice, \"prompt1.txt\")  # 기본값으로 prompt1.txt 사용\n",
    "\n",
    "prompt_file = select_prompt()\n",
    "\n",
    "# 선택한 프롬프트 파일 읽기\n",
    "with open(prompt_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    system_message = file.read()\n",
    "\n",
    "# ChatPromptTemplate 구성\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_message_template = SystemMessagePromptTemplate.from_template(system_message)\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_template, human_message_template]\n",
    ")\n",
    "\n",
    "# RAG 체인 클래스 및 디버그 기능 추가\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import LLMChain\n",
    "from datetime import datetime\n",
    "\n",
    "class DebugPassThrough(RunnablePassthrough):\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        output = super().invoke(*args, **kwargs)\n",
    "        print(\"Debug Output:\", output)\n",
    "        return output\n",
    "\n",
    "class ContextToText:\n",
    "    def invoke(self, inputs, config=None, **kwargs):\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in inputs[\"context\"]])\n",
    "        return {\"context\": context_text, \"question\": inputs[\"question\"]}\n",
    "\n",
    "contextual_prompt = LLMChain(\n",
    "    prompt=chat_prompt,\n",
    "    llm=model,\n",
    ")\n",
    "\n",
    "# 질문 응답 체인 구성\n",
    "def get_contextual_response(query):\n",
    "    # 리트리버에서 컨텍스트 가져오기\n",
    "    context = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # 문서 내용을 텍스트로 변환\n",
    "    context_text = \"\\n\".join([doc.page_content for doc in context])\n",
    "\n",
    "    # 최종 시스템 및 사용자 메시지 생성\n",
    "    formatted_messages = chat_prompt.format_messages(\n",
    "        context=context_text, user_input=query\n",
    "    )\n",
    "\n",
    "    # 모델 실행\n",
    "    response = DebugPassThrough().invoke(model(formatted_messages))\n",
    "    return response\n",
    "\n",
    "# 결과 저장 경로\n",
    "output_folder = r\"C:\\Users\\USER\\mission\\mission\\Results\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # 폴더가 없으면 생성\n",
    "\n",
    "# 질문 입력 및 처리\n",
    "query = input(\"질문을 입력하세요: \")\n",
    "\n",
    "# 모델로부터 답변 생성\n",
    "response = get_contextual_response(query)\n",
    "\n",
    "# 답변 출력\n",
    "print(\"Final Response: \")\n",
    "print(response.content)\n",
    "\n",
    "# 결과 파일 저장\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # 현재 시각으로 파일명 생성\n",
    "output_file = os.path.join(output_folder, f\"response_{timestamp}.txt\")\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(f\"질문: {query}\\n\\n\")\n",
    "    file.write(f\"답변:\\n{response.content}\\n\")\n",
    "\n",
    "print(f\"답변이 다음 위치에 저장되었습니다: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6e1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
