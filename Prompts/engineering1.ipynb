{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "886f1233",
   "metadata": {},
   "source": [
    "## 수정 사항\n",
    "1. 백업 스토어 설정에서 InMemoryDocstore를 사용해보기\n",
    "2. 파라미터 조정\n",
    "3. 프롬프트에 명령 추가\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ae6398",
   "metadata": {},
   "source": [
    "## 1. 백터 스토어 설정에서 InMemoryDocstore를 사용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98423e2",
   "metadata": {},
   "source": [
    "## 2. 파라미터 조정\n",
    "\n",
    "- 모델 지정에서 파라미터 추가(temperature, max_tokens)\n",
    "- 리트리버 변환에서 k값 변경(1->2)\n",
    "- 모델 로드에 UTF-8 추가\n",
    "\n",
    "\n",
    "### 리트리버 파라미터 설명\n",
    "- search_type\n",
    " - similarity: 벡터간 유사도, 내적 사용 검색\n",
    " - mmr: 다양성 고려 검색\n",
    " - approx: 근사적 검색(대규모 데이터셋에서 빠른 검색)\n",
    "\n",
    "- search_kwargs\n",
    " - k: 반환할 결과의 수 \n",
    " - lambda_mult: 유사도와 다양성 간의 균형 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ef608b",
   "metadata": {},
   "source": [
    "# 수정 본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cdb4302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key 입력: ········\n",
      "=====================\n",
      "질문을 입력하세요: 최근 초대형 언어모델 주요 연구동향을 알려줘\n",
      "Debug Output: 최근 초대형 언어모델 주요 연구동향을 알려줘\n",
      "Debug Output: {'context': [], 'question': '최근 초대형 언어모델 주요 연구동향을 알려줘'}\n",
      "Final Response: \n",
      "최근 초대형 언어모델(LLM) 연구동향은 여러 가지 중요한 발전과 도전 과제를 포함하고 있습니다. 다음은 주요 연구 동향과 발견 사항에 대한 종합적인 분석입니다.\n",
      "\n",
      "1. **모델 크기와 성능**: 초대형 언어모델의 크기가 커짐에 따라 성능이 향상되는 경향이 있습니다. 연구자들은 파라미터 수가 많은 모델이 더 복잡한 언어 패턴을 학습하고 더 정교한 응답을 생성할 수 있다는 것을 발견했습니다. 그러나 모델의 크기가 증가함에 따라 훈련 비용과 에너지 소비도 비례적으로 증가하는 문제도 제기되고 있습니다.\n",
      "\n",
      "2. **효율적인 훈련 기법**: 최근 연구에서는 훈련 효율성을 높이기 위한 다양한 기법들이 개발되고 있습니다. 예를 들어, 혼합 정밀도 훈련(mixed-precision training)과 같은 기술이 도입되어 메모리 사용량을 줄이고 훈련 속도를 높이는 데 기여하고 있습니다. 또한, 분산 훈련 기법이 발전하여 대규모 데이터셋을 효과적으로 처리할 수 있는 방법이 모색되고 있습니다.\n",
      "\n",
      "3. **전이 학습과 파인튜닝**: 초대형 언어모델은 전이 학습을 통해 다양한 태스크에 적응할 수 있는 능력을 보여주고 있습니다. 연구자들은 사전 훈련된 모델을 특정 도메인이나 태스크에 맞게 파인튜닝하는 방법을 통해 성능을 극대화하고 있습니다. 이 과정에서 적은 양의 데이터로도 높은 성능을 달성할 수 있는 가능성이 열리고 있습니다.\n",
      "\n",
      "4. **윤리적 고려사항**: 초대형 언어모델의 사용이 증가함에 따라 윤리적 문제도 중요한 연구 주제로 떠오르고 있습니다. 편향(bias), 개인정보 보호, 그리고 모델의 오용 가능성에 대한 우려가 커지고 있으며, 이를 해결하기 위한 다양한 접근법이 논의되고 있습니다. 연구자들은 공정성과 투명성을 높이기 위한 방법론을 개발하고 있습니다.\n",
      "\n",
      "5. **모델의 해석 가능성**: 초대형 언어모델의 복잡성으로 인해 모델의 결정 과정과 결과를 해석하는 것이 어려워지고 있습니다. 이에 따라 모델의 해석 가능성을 높이기 위한 연구가 활발히 진행되고 있으며, 이는 AI 시스템의 신뢰성을 높이는 데 중요한 역할을 하고 있습니다.\n",
      "\n",
      "6. **응용 분야의 확장**: 초대형 언어모델은 자연어 처리(NLP) 분야를 넘어 다양한 분야에서 응용되고 있습니다. 예를 들어, 의료, 법률, 교육 등 여러 산업에서 LLM을 활용한 혁신적인 솔루션이 개발되고 있습니다. 이러한 응용은 모델의 실제 사용 사례를 다양화하고 있습니다.\n",
      "\n",
      "결론적으로, 초대형 언어모델 연구는 성능 향상, 훈련 효율성, 윤리적 고려사항, 해석 가능성, 그리고 다양한 응용 분야로의 확장을 중심으로 발전하고 있습니다. 이러한 동향은 앞으로의 AI 기술 발전에 중요한 영향을 미칠 것으로 예상됩니다.\n",
      "=====================\n",
      "질문을 입력하세요: exit\n",
      "Debug Output: exit\n",
      "Debug Output: {'context': [], 'question': 'exit'}\n",
      "Final Response: \n",
      "The document on super-large language models outlines several key trends and research findings that are shaping the field. Here’s a comprehensive analysis based on the latest advancements, challenges, and contributions:\n",
      "\n",
      "### Key Trends in Super-Large Language Models\n",
      "\n",
      "1. **Scale and Performance**: One of the most significant trends is the continuous increase in the scale of language models. Researchers are pushing the boundaries of model size, with architectures reaching hundreds of billions or even trillions of parameters. This scaling has been shown to correlate with improved performance on a variety of natural language processing (NLP) tasks, including text generation, translation, and comprehension.\n",
      "\n",
      "2. **Architectural Innovations**: Alongside scaling, there have been notable innovations in model architecture. Techniques such as sparse attention mechanisms, mixture of experts, and efficient transformer variants are being explored to enhance computational efficiency and reduce the resource requirements for training and inference.\n",
      "\n",
      "3. **Fine-tuning and Adaptation**: There is a growing emphasis on fine-tuning super-large models for specific tasks or domains. Researchers are developing methods to adapt pre-trained models to specialized applications, which can lead to significant performance gains while minimizing the need for extensive retraining.\n",
      "\n",
      "4. **Multimodal Capabilities**: Recent advancements have also seen the integration of multimodal capabilities, where models can process and generate not just text but also images, audio, and other data types. This trend is paving the way for more versatile AI systems that can understand and generate content across different modalities.\n",
      "\n",
      "5. **Ethical Considerations and Bias Mitigation**: As models grow in size and capability, there is an increasing focus on the ethical implications of their deployment. Researchers are investigating methods to identify and mitigate biases in training data and model outputs, as well as exploring frameworks for responsible AI use.\n",
      "\n",
      "### Challenges in Super-Large Language Models\n",
      "\n",
      "1. **Resource Intensity**: Training and deploying super-large models require substantial computational resources, leading to concerns about environmental impact and accessibility. Researchers are actively seeking ways to optimize training processes and reduce the carbon footprint associated with these models.\n",
      "\n",
      "2. **Interpretability and Transparency**: As models become more complex, understanding their decision-making processes becomes increasingly challenging. There is a pressing need for tools and methodologies that enhance the interpretability of model predictions, allowing users to trust and understand AI outputs.\n",
      "\n",
      "3. **Generalization and Robustness**: While larger models tend to perform better on benchmark tasks, their ability to generalize to unseen data or adversarial inputs remains a concern. Ongoing research is focused on improving the robustness of these models to ensure reliable performance in real-world applications.\n",
      "\n",
      "4. **Data Privacy and Security**: The use of vast datasets for training raises questions about data privacy and security. Researchers are exploring techniques such as differential privacy and federated learning to safeguard sensitive information while still benefiting from large-scale data.\n",
      "\n",
      "### Key Contributions\n",
      "\n",
      "1. **Open-Source Initiatives**: The open-source movement has played a crucial role in democratizing access to super-large language models. Initiatives like Hugging Face and EleutherAI have made significant contributions by providing pre-trained models and tools that allow researchers and developers to experiment and build upon existing work.\n",
      "\n",
      "2. **Benchmarking and Evaluation**: New benchmarks and evaluation metrics are being developed to better assess the performance of super-large models across diverse tasks. These efforts aim to create a more standardized approach to evaluating model capabilities and limitations.\n",
      "\n",
      "3. **Interdisciplinary Collaboration**: The field is witnessing increased collaboration between AI researchers and experts from other domains, such as linguistics, cognitive science, and ethics. This interdisciplinary approach is enriching the research landscape and fostering innovative solutions to complex challenges.\n",
      "\n",
      "In summary, the research landscape for super-large language models is dynamic and rapidly evolving, characterized by significant advancements in scale and capability, alongside pressing challenges that require careful consideration. The contributions from the community, both in terms of technical innovations and ethical frameworks, are essential for guiding the future development of these powerful AI systems.\n",
      "=====================\n",
      "질문을 입력하세요: exit\n",
      "Debug Output: exit\n",
      "Debug Output: {'context': [], 'question': 'exit'}\n",
      "Final Response: \n",
      "The document highlights several key trends and research findings in the realm of super-large language models (LLMs), which have become increasingly prominent in the field of artificial intelligence. Here’s a comprehensive analysis of the advancements, challenges, and contributions outlined in the document:\n",
      "\n",
      "### Key Trends in Super-Large Language Models\n",
      "\n",
      "1. **Scale and Performance**: \n",
      "   - The trend toward larger models has been a defining characteristic of recent advancements. Researchers have found that increasing the number of parameters in LLMs often correlates with improved performance on a variety of natural language processing (NLP) tasks. This has led to models with hundreds of billions, or even trillions, of parameters.\n",
      "\n",
      "2. **Architecture Innovations**: \n",
      "   - Innovations in model architecture, such as the introduction of sparse attention mechanisms and efficient transformer variants, have emerged to address the computational challenges associated with training and deploying super-large models. These architectures aim to reduce the memory footprint and increase the speed of training and inference.\n",
      "\n",
      "3. **Multi-Modal Capabilities**: \n",
      "   - There is a growing trend towards developing models that can handle multiple types of data inputs, such as text, images, and audio. This multi-modal approach enhances the models' ability to understand and generate content across different formats, leading to more versatile applications.\n",
      "\n",
      "4. **Fine-Tuning and Transfer Learning**: \n",
      "   - Fine-tuning pre-trained models on specific tasks has become a standard practice, allowing researchers and practitioners to leverage the vast knowledge encoded in super-large models while tailoring them for particular applications. This has resulted in significant improvements in task-specific performance.\n",
      "\n",
      "5. **Ethical Considerations and Bias Mitigation**: \n",
      "   - As the capabilities of LLMs expand, so do concerns regarding ethical implications, including biases present in training data. Researchers are increasingly focused on developing methodologies to identify, mitigate, and address biases in LLMs to ensure fair and responsible AI deployment.\n",
      "\n",
      "### Challenges Faced by Super-Large Language Models\n",
      "\n",
      "1. **Resource Intensity**: \n",
      "   - Training super-large models requires substantial computational resources, which can be a barrier to entry for many researchers and organizations. The environmental impact of such resource consumption is also a growing concern.\n",
      "\n",
      "2. **Interpretability and Transparency**: \n",
      "   - Understanding the decision-making processes of LLMs remains a significant challenge. As models grow in complexity, interpreting their outputs and ensuring transparency becomes increasingly difficult, hindering trust and accountability.\n",
      "\n",
      "3. **Data Privacy and Security**: \n",
      "   - The use of large datasets for training raises concerns about data privacy and the potential for models to inadvertently memorize sensitive information. Developing strategies to ensure data security while maintaining model performance is an ongoing area of research.\n",
      "\n",
      "4. **Generalization vs. Overfitting**: \n",
      "   - While larger models can capture more complex patterns, they also risk overfitting to the training data. Striking a balance between generalization and specificity is crucial for the effective deployment of LLMs in real-world applications.\n",
      "\n",
      "### Key Contributions to the Field\n",
      "\n",
      "1. **Benchmarking and Evaluation**: \n",
      "   - The establishment of standardized benchmarks for evaluating the performance of LLMs has been a significant contribution. These benchmarks help in comparing models across different tasks and facilitate the identification of strengths and weaknesses.\n",
      "\n",
      "2. **Open-Source Initiatives**: \n",
      "   - The rise of open-source models and collaborative research efforts has democratized access to advanced LLMs. This trend encourages innovation and allows a broader community of researchers to contribute to the field.\n",
      "\n",
      "3. **Interdisciplinary Research**: \n",
      "   - The intersection of NLP with fields such as cognitive science, linguistics, and ethics has led to richer insights and more holistic approaches to understanding language and its complexities.\n",
      "\n",
      "4. **Applications in Industry**: \n",
      "   - The practical applications of super-large language models have expanded significantly, with industries such as healthcare, finance, and education leveraging these models for tasks ranging from automated customer support to advanced data analysis.\n",
      "\n",
      "In conclusion, the document outlines a dynamic landscape in the research of super-large language models, characterized by rapid advancements and significant challenges. As the field continues to evolve, ongoing research will likely focus on enhancing model efficiency, addressing ethical concerns, and exploring innovative applications across various domains.\n",
      "=====================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=====================\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m질문을 입력하세요: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     response \u001b[38;5;241m=\u001b[39m rag_chain_debug\u001b[38;5;241m.\u001b[39minvoke(query)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Response: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# KEY설정\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key 입력: \")\n",
    "\n",
    "# 모델로드\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = ChatOpenAI(model = \"gpt-4o-mini\", temperature = 0.3, max_tokens = 1000)\n",
    "\n",
    "# 파일 로드\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = \"초거대언어모델연구동향.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "utf8_docs = [doc.page_content.encode('utf-8').decode('utf-8') for doc in docs]\n",
    "\n",
    "# 문서 청크 나누기 2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = recursive_text_splitter.split_documents(docs)\n",
    "\n",
    "    \n",
    "# 벡터 임베딩 생성\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model = \"text-embedding-ada-002\")\n",
    "\n",
    "# 벡터 스토어 생성\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "\n",
    "vector_dim = len(embeddings.embed_query(\"example text\")) \n",
    "index = faiss.IndexFlatL2(vector_dim)\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "for split in splits:\n",
    "    vector_store.add_texts([split.page_content], metadatas=[split.metadata])\n",
    "\n",
    "# 리트리버 변환\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "# 템플릿 정의\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an artificial intelligence model researcher. The document describes the latest research trends in super-large language models, including advancements, challenges, and key contributions. Your task is to analyze the document and provide a detailed and insightful response.\"),\n",
    "    (\"user\", \"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer: Please provide a comprehensive response, focusing on the key trends and research findings mentioned in the document.\")\n",
    "])\n",
    "\n",
    "# 질문 응답 체인 구성\n",
    "class DebugPassThrough(RunnablePassthrough):\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        output = super().invoke(*args, **kwargs)\n",
    "        print(\"Debug Output:\", output)\n",
    "        return output\n",
    "\n",
    "class ContextToText(RunnablePassthrough):\n",
    "    def invoke(self, inputs, config=None, **kwargs):\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in inputs[\"context\"]])\n",
    "        return {\"context\": context_text, \"question\": inputs[\"question\"]}\n",
    "\n",
    "# 질문 반복 처리\n",
    "rag_chain_debug = {\n",
    "    \"context\": retriever,   # 컨텍스트를 가져오는 retriever\n",
    "    \"question\": DebugPassThrough()     # 사용자 질문이 그대로 전달되는지 확인하는 passthrough\n",
    "}  | DebugPassThrough() | ContextToText()| contextual_prompt | model\n",
    "\n",
    "# 출력\n",
    "    print('=====================')\n",
    "    query = input(\"질문을 입력하세요: \")\n",
    "\n",
    "    response = rag_chain_debug.invoke(query)\n",
    "    \n",
    "    print(\"Final Response: \")\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b608c6e",
   "metadata": {},
   "source": [
    "# 1.\n",
    "- 질문: 최근 초거대 언어모델 연구동향을 알려줘\n",
    "- 대답: 깔끔하게 정리 되었고 내용에 맞는 대답을 한다.\n",
    "\n",
    "# 2. \n",
    "- 질문: 각 주제들 중에 가장 중요하다고 생각드는건 무엇이며 왜그런지 설명해주세요\n",
    "- 대답: 갑자기 영어로 대답을하여 놀랐다. 내용은 초대형 언어 모델 배포의 윤리적 영향과 사회적 영향을 말하고있다.\n",
    "\n",
    "# 3.\n",
    "- 질문: 방금전 글을 한글로 말해줘\n",
    "- 대답: 첫 번째 질문을 그대로 가져왔다. \n",
    "\n",
    "# 4.\n",
    "- 질문: 각 주제들 중에 가장 중요하다고 생각드는건 무엇이며 왜그런지 한글로 설명해주세요 (한글로 설명 부탁)\n",
    "- 대답: 질문에 관련없는 내용이 나옴\n",
    "\n",
    "\n",
    "## 프롬프트에서 바꾸어야할 점\n",
    "\n",
    "### 이전 프롬프트\n",
    "- system: 인공지능 모델 연구자이십니다. 이 문서에서는 발전, 과제 및 주요 기여를 포함하여 초대형 언어 모델의 최신 연구 동향을 설명합니다. 귀하의 임무는 문서를 분석하고 상세하고 통찰력 있는 답변을 제공하는 것입니다.\n",
    "- user: 본 문서에 언급된 주요 동향과 연구 결과를 중심으로 포괄적인 답변을 부탁드립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eed056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
