{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa9495d",
   "metadata": {},
   "source": [
    "# 시스템 설정\n",
    "1. 역할: 귀하의 임무는 문서를 분석하고 상세하고 통찰력 있는 답변을 제공하는 인공지능 모델 연구 강연자이십니다. \n",
    "2. 참고 문서: {retriever}\n",
    "3. 강연 대상: 강연 대상은 불특정 다수로, 초대형 언어 모델을 모르는 사람도 존재합니다. 이런 경우를 고려하여 대답해야합니다. \n",
    "4. 추가 고려 사항: 질문을 받고 답변을 이해하기 쉽게 답변을 하기 전 부연 설명을 해줘, 이때 참고 문서의 내용을 먼저 고려하고 내용이 부족하다고 판단 되면 추가 구글을 통해 검색을 진행해서 답변을 진행해야합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = f\"\"\"\n",
    "1. Your role: Your job is to analyze the document and provide detailed and insightful answers as a speaker on AI model research. \n",
    "2. reference document: {retriever}\n",
    "3. Audience: Your audience is unspecified, and some people may not know about very large language models. You will have to answer considering this case. \n",
    "4. additional considerations: When you are asked a question, you should give an explanation before answering to make your answer easy to understand, so you should consider the content of the reference document first, and if you find the content insufficient, you should do additional Google searches before answering. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176920e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEY설정\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key 입력: \")\n",
    "\n",
    "# 모델로드\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = ChatOpenAI(model = \"gpt-4o-mini\", temperature = 0.3, max_tokens = 1000)\n",
    "\n",
    "# 파일 로드\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = \"초거대언어모델연구동향.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "for doc in docs:\n",
    "    utf8_docs = [doc.page_content.encode('utf-8').decode('utf-8') for doc in docs]\n",
    "\n",
    "# 문서 청크 나누기 2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = recursive_text_splitter.split_documents(docs)\n",
    "\n",
    "    \n",
    "# 벡터 임베딩 생성\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model = \"text-embedding-ada-002\")\n",
    "\n",
    "# 벡터 스토어 생성\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "\n",
    "vector_dim = len(embeddings.embed_query(\"example text\")) \n",
    "index = faiss.IndexFlatL2(vector_dim)\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "# 리트리버 변환\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "# 템플릿 정의\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 시스템 설정\n",
    "System = f\"\"\"\n",
    "1. Your role: Your job is to analyze the document and provide detailed and insightful answers as a speaker on AI model research. \n",
    "2. reference document: {retriever}\n",
    "3. Audience: Your audience is unspecified, and some people may not know about very large language models. You will have to answer considering this case. \n",
    "4. additional considerations: When you are asked a question, you should give an explanation before answering to make your answer easy to understand, so you should consider the content of the reference document first, and if you find the content insufficient, you should do additional Google searches before answering. \n",
    "\"\"\"\n",
    "\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", System),\n",
    "    (\"user\", \"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer: Please provide a comprehensive response, focusing on the key trends and research findings mentioned in the document.\")\n",
    "])\n",
    "\n",
    "# 질문 응답 체인 구성\n",
    "class DebugPassThrough(RunnablePassthrough):\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        output = super().invoke(*args, **kwargs)\n",
    "        print(\"Debug Output:\", output)\n",
    "        return output\n",
    "\n",
    "class ContextToText(RunnablePassthrough):\n",
    "    def invoke(self, inputs, config=None, **kwargs):\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in inputs[\"context\"]])\n",
    "        return {\"context\": context_text, \"question\": inputs[\"question\"]}\n",
    "\n",
    "# 질문 반복 처리\n",
    "rag_chain_debug = {\n",
    "    \"context\": retriever,   # 컨텍스트를 가져오는 retriever\n",
    "    \"question\": DebugPassThrough()     # 사용자 질문이 그대로 전달되는지 확인하는 passthrough\n",
    "}  | DebugPassThrough() | ContextToText()| contextual_prompt | model\n",
    "\n",
    "# 출력\n",
    "while True:\n",
    "    print('=====================')\n",
    "    query = input(\"질문을 입력하세요: \")\n",
    "\n",
    "    response = rag_chain_debug.invoke(query)\n",
    "    \n",
    "    print(\"Final Response: \")\n",
    "    print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
