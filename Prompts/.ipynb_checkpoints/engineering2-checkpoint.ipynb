{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdf6cd16",
   "metadata": {},
   "source": [
    "# 수정 및 추가\n",
    "\n",
    "## 1. txt파일로 프롬프트 적용\n",
    "1. select_prompt 정의(items, get, open, read)\n",
    " - items: 키, 값을 튜플로 묶어 반환\n",
    " - get: choice(사용자 입력 값)을 키로 값(prompt)를 반환\n",
    " - open:파일 열어서 읽기 모드('r')\n",
    " - read: 파일 불러오기\n",
    "2. langchain.prompts.chat 정의(ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate )\n",
    " - ChatPromptTemplate: 다른 두 템플릿을 결합해서 대화형식 메시지로 구성\n",
    " - SystemMessagePromptTemplate: 모델의 역할(system_message) 지정\n",
    " - HumanMessagePromptTemplate: 사용자의 질문({user_input}) 지정\n",
    " \n",
    " \n",
    "## 2. RAG체인 구성 변환\n",
    "\n",
    "1) LLMChain 추가\n",
    " - 템플릿을 기반으로 여러 메시지를 조합하고 모델에 전달\n",
    " - contextual_prompt을 처리하고, 모델을 실행하는 방식으로 변경\n",
    "\n",
    "\n",
    "2) ContextToText 클래스 수정\n",
    " - LLMChain에 맞게 RunnablePassthrough를 상속 받지않고, 단순 메서드 정의\n",
    "\n",
    "3) 실행방식 변경(파이프라인=>get_contextual_response())\n",
    " - 사용자의 질문에 대한 컨텍스트를 가져오고, 이를 chat_prompt 템플릿에 맞게 포맷팅하여 모델에 전달\n",
    " - retriever.get_relevant_documents(query): 질문에 맞는 문서들을 검색\n",
    " - ContextToText 클래스에서 변환된 텍스트는 formatted_messages에 담겨 모델에 전달\n",
    "\n",
    "## 3. 반복 질문 삭제\n",
    "- while루프 제거\n",
    "\n",
    "## 4. 답변 txt파일로 저장 코드 추가\n",
    "- output_folder: r포맷팅으로 저장 경로 지정\n",
    "- os.makedirs: 폴더안에 저장 지정\n",
    "- timestamp: 현재 년,월,일,시,분,초를 txt파일 이름으로 지정\n",
    "- open: 쓰기모드('w\")를 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61f785",
   "metadata": {},
   "source": [
    "# 수정 본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6660f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key 입력: ········\n",
      "=====================\n",
      "질문을 입력하세요: 초거대 언어모델의 주요 연구동향을 설명해줘\n",
      "Debug Output: 초거대 언어모델의 주요 연구동향을 설명해줘\n",
      "Debug Output: {'context': [], 'question': '초거대 언어모델의 주요 연구동향을 설명해줘'}\n",
      "Final Response: \n",
      "초거대 언어모델(very large language models, VLLMs)은 인공지능(AI) 분야에서 최근 몇 년간 큰 주목을 받고 있는 연구 주제입니다. 이러한 모델들은 방대한 양의 텍스트 데이터를 학습하여 자연어 처리(NLP) 작업을 수행하는 데 사용됩니다. 여기서는 초거대 언어모델의 주요 연구 동향을 설명하겠습니다.\n",
      "\n",
      "1. **모델 크기와 성능의 관계**: 초거대 언어모델은 일반적으로 수십억 개의 매개변수를 가지고 있으며, 이는 모델의 성능을 크게 향상시킵니다. 연구자들은 모델의 크기를 늘리는 것이 성능 개선에 미치는 영향을 지속적으로 조사하고 있습니다. 예를 들어, OpenAI의 GPT-3와 같은 모델은 그 크기 덕분에 다양한 언어 작업에서 뛰어난 성능을 보여주었습니다.\n",
      "\n",
      "2. **효율성 및 지속 가능성**: 모델의 크기가 커짐에 따라 학습과 추론에 필요한 계산 자원도 증가합니다. 이에 따라 연구자들은 더 적은 자원으로도 효과적으로 학습할 수 있는 방법을 모색하고 있습니다. 예를 들어, 지식 증류(knowledge distillation)와 같은 기술을 통해 작은 모델이 큰 모델의 성능을 모방할 수 있도록 하는 연구가 활발히 진행되고 있습니다.\n",
      "\n",
      "3. **다양한 응용 분야**: 초거대 언어모델은 단순한 텍스트 생성뿐만 아니라, 번역, 요약, 질문 응답 등 다양한 NLP 작업에 활용되고 있습니다. 최근 연구에서는 이러한 모델들이 특정 도메인에 맞춰 조정(파인튜닝)될 수 있도록 하는 방법에 대한 관심도 높아지고 있습니다.\n",
      "\n",
      "4. **윤리적 고려사항**: 초거대 언어모델의 사용이 증가함에 따라, 이들 모델이 생성하는 콘텐츠의 윤리적 측면에 대한 논의도 활발히 이루어지고 있습니다. 예를 들어, 편향된 데이터로 학습된 모델이 편향된 결과를 생성할 수 있다는 우려가 있으며, 이를 해결하기 위한 방법론이 연구되고 있습니다.\n",
      "\n",
      "5. **인터랙티브 AI 시스템**: 최근에는 초거대 언어모델을 기반으로 한 대화형 AI 시스템의 개발이 활발히 이루어지고 있습니다. 이러한 시스템은 사용자와의 상호작용을 통해 더욱 자연스럽고 유용한 대화를 생성할 수 있도록 설계되고 있습니다.\n",
      "\n",
      "이러한 동향들은 초거대 언어모델이 앞으로도 계속해서 발전하고, 다양한 분야에서 활용될 수 있는 가능성을 보여줍니다. 연구자들은 이 모델들이 가진 잠재력을 최대한 활용하기 위해 지속적으로 새로운 방법과 기술을 개발하고 있습니다.\n",
      "=====================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 99\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=====================\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m질문을 입력하세요: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     response \u001b[38;5;241m=\u001b[39m rag_chain_debug\u001b[38;5;241m.\u001b[39minvoke(query)\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Response: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# OpenAI API Key 설정\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key 입력: \")\n",
    "\n",
    "# 모델 로드\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3, max_tokens=1000)\n",
    "\n",
    "# PDF 파일 로드\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = \"초거대언어모델연구동향.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서 청크 나누기\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = recursive_text_splitter.split_documents(docs)\n",
    "\n",
    "# 벡터 임베딩 생성\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vector_dim = len(embeddings.embed_query(\"example text\"))\n",
    "index = faiss.IndexFlatL2(vector_dim)\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "# 문서 추가\n",
    "for split in splits:\n",
    "    vector_store.add_texts([split.page_content], metadatas=[split.metadata])\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "\n",
    "# 프롬프트 파일 선택\n",
    "def select_prompt():\n",
    "    prompts = {\n",
    "        \"1\": \"prompt1.txt\",\n",
    "        \"2\": \"prompt2.txt\",\n",
    "        \"3\": \"prompt3.txt\",\n",
    "    }\n",
    "\n",
    "    print(\"사용할 프롬프트를 선택하세요:\")\n",
    "    for key, value in prompts.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    choice = input(\"선택: \").strip()\n",
    "    return prompts.get(choice, \"prompt1.txt\")  # 기본값으로 prompt1.txt 사용\n",
    "\n",
    "prompt_file = select_prompt()\n",
    "\n",
    "# 선택한 프롬프트 파일 읽기\n",
    "with open(prompt_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    system_message = file.read()\n",
    "\n",
    "# ChatPromptTemplate 구성\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_message_template = SystemMessagePromptTemplate.from_template(system_message)\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_template, human_message_template]\n",
    ")\n",
    "\n",
    "# RAG 체인 클래스 및 디버그 기능 추가\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.chains import LLMChain\n",
    "from datetime import datetime\n",
    "\n",
    "class DebugPassThrough(RunnablePassthrough):\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        output = super().invoke(*args, **kwargs)\n",
    "        print(\"Debug Output:\", output)\n",
    "        return output\n",
    "\n",
    "class ContextToText:\n",
    "    def invoke(self, inputs, config=None, **kwargs):\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in inputs[\"context\"]])\n",
    "        return {\"context\": context_text, \"question\": inputs[\"question\"]}\n",
    "\n",
    "contextual_prompt = LLMChain(\n",
    "    prompt=chat_prompt,\n",
    "    llm=model,\n",
    ")\n",
    "\n",
    "# 질문 응답 체인 구성\n",
    "def get_contextual_response(query):\n",
    "    # 리트리버에서 컨텍스트 가져오기\n",
    "    context = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # 문서 내용을 텍스트로 변환\n",
    "    context_text = \"\\n\".join([doc.page_content for doc in context])\n",
    "\n",
    "    # 최종 시스템 및 사용자 메시지 생성\n",
    "    formatted_messages = chat_prompt.format_messages(\n",
    "        context=context_text, user_input=query\n",
    "    )\n",
    "\n",
    "    # 모델 실행\n",
    "    response = DebugPassThrough().invoke(model(formatted_messages))\n",
    "    return response\n",
    "\n",
    "# 결과 저장 경로\n",
    "output_folder = r\"C:\\Users\\USER\\mission\\mission\\Results\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # 폴더가 없으면 생성\n",
    "\n",
    "# 질문 입력 및 처리\n",
    "query = input(\"질문을 입력하세요: \")\n",
    "\n",
    "# 모델로부터 답변 생성\n",
    "response = get_contextual_response(query)\n",
    "\n",
    "# 답변 출력\n",
    "print(\"Final Response: \")\n",
    "print(response.content)\n",
    "\n",
    "# 결과 파일 저장\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # 현재 시각으로 파일명 생성\n",
    "output_file = os.path.join(output_folder, f\"response_{timestamp}.txt\")\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(f\"질문: {query}\\n\\n\")\n",
    "    file.write(f\"답변:\\n{response.content}\\n\")\n",
    "\n",
    "print(f\"답변이 다음 위치에 저장되었습니다: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
